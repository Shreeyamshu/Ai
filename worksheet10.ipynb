{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Workshop 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk, re, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
                "from nltk.corpus import movie_reviews, stopwords\n",
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.metrics import *\n",
                "from sklearn.datasets import fetch_openml\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.feature_selection import RFE\n",
                "nltk.download('movie_reviews'); nltk.download('stopwords'); nltk.download('punkt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### EX 1 - Sentiment Analysis (Section 2 in PDF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "docs = [(\" \".join(movie_reviews.words(f)), c) for c in movie_reviews.categories() for f in movie_reviews.fileids(c)]\n",
                "df = pd.DataFrame(docs, columns=['r', 's'])\n",
                "st, sw = PorterStemmer(), set(stopwords.words('english'))\n",
                "def proc(t):\n",
                "    t = re.sub(r'[^a-zA-Z\\s]', '', t.lower())\n",
                "    return \" \".join([st.stem(w) for w in word_tokenize(t) if w not in sw])\n",
                "df['pr'] = df['r'].apply(proc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xt, xv, yt, yv = train_test_split(df['pr'], df['s'], test_size=0.2, random_state=42)\n",
                "cv = CountVectorizer()\n",
                "xt_b, xv_b = cv.fit_transform(xt), cv.transform(xv)\n",
                "nb = MultinomialNB().fit(xt_b, yt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "yp = nb.predict(xv_b)\n",
                "yprob = nb.predict_proba(xv_b)[:, 1]\n",
                "print(f\"Acc: {accuracy_score(yv, yp):.4f}\")\n",
                "print(classification_report(yv, yp))\n",
                "sns.heatmap(confusion_matrix(yv, yp), annot=True, fmt='d'); plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### EX 3 - Feature Selection (Section 3 in PDF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bc = fetch_openml(data_id=35, as_frame=True).frame.dropna()\n",
                "X, y = bc.iloc[:, :-1], LabelEncoder().fit_transform(bc.iloc[:, -1])\n",
                "Xs = StandardScaler().fit_transform(X)\n",
                "xt2, xv2, yt2, yv2 = train_test_split(Xs, y, test_size=0.2, random_state=42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr = LogisticRegression(max_iter=1000)\n",
                "rfe = RFE(estimator=lr, n_features_to_select=5).fit(xt2, yt2)\n",
                "print(f\"top 5: {X.columns[rfe.support_].tolist()}\")\n",
                "plt.barh(X.columns, rfe.ranking_); plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def eval(xtr, xte, ytr, yte, l):\n",
                "    m = LogisticRegression(max_iter=1000).fit(xtr, ytr)\n",
                "    p = m.predict(xte)\n",
                "    return {'l': l, 'acc': accuracy_score(yte, p), 'f1': f1_score(yte, p)}\n",
                "res = [eval(xt2, xv2, yt2, yv2, \"all\")]\n",
                "for n in [5, 3, 7]:\n",
                "    rf = RFE(estimator=lr, n_features_to_select=n).fit(xt2, yt2)\n",
                "    res.append(eval(rf.transform(xt2), rf.transform(xv2), yt2, yv2, f\"top {n}\"))\n",
                "print(pd.DataFrame(res))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Discussion: Filtering out junk data with feature selection helps the model stay focused and efficient."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}